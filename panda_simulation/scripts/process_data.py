#! /usr/bin/env python
# -*- coding: utf-8 -*-

#uni: University of Cape Town
#author: Noel Loxton
#student ID: LXTNOE001
#This code forms part of the EEE4022S honours thesis project, my project is Visual Servoing of a Simulated Robotic Arm
#This file is used to process datasets generated by the generate_data.py python script for input to a tensorflow/keras CNN model

import numpy as np
import os
import tensorflow as tf
import tensorflow_datasets as tfds
import pandas as pd
from keras import models, optimizers, backend
from keras.layers import core, convolutional, pooling
from sklearn import model_selection

print(tf.__version__)

#delete redundant "camera.jpeg" file
deleteCommand = "rm -r ~/arm_testing_ws/src/panda_simulation/data_for_nn/camera_image.jpeg"
os.system(deleteCommand)

#make an array of filenames
path = '/home/ghostlini/arm_testing_ws/src/panda_simulation/data_for_nn'
with open((path+'/label.txt'), 'r') as f:
    	labels_list = f.read().splitlines()
    	f.close()
labels_list = [float(i) for i in labels_list]
filenames_list = sorted(os.listdir(path))
filenames_list.remove('label.txt')
filenames = tf.constant(filenames_list)
labels = tf.constant(labels_list)
#print(filenames)
#print(labels)

dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
#print(dataset)

for element in dataset:
	print(element)

"""
def im_file_to_tensor(file, label):
	def _im_file_to_tensor(file, label):
		pathtofile = f"/home/ghostlini/arm_testing_ws/src/panda_simulation/data_for_nn/{file.numpy().decode()}"
		im = tf.image.decode_jpeg(tf.io.read_file(pathtofile), channels=3)
		im = tf.cast(image_decoded, tf.float32)/255.0
		return im, label
	file, label = tf.py_function(_im_file_to_tensor, inp=(file, label), Tout=(tf.float32, tf.uint8))
	file.set_shape([192, 192, 3])
	label.set_shape([])
	return (file, label)
	#return tf.py_function(_im_file_to_tensor, inp=(file, label), Tout=(tf.float32, tf.uint8))

dataset = dataset.map(im_file_to_tensor)
print(dataset)
"""
#train_dataset = dataset.sample(frac=0.8, random_state=0)
#test_dataset = dataset.drop(train_dataset.index)
#dataset = dataset_train, dataset_valid = model_selection.train_test_split(dataset, test_size=.2)
